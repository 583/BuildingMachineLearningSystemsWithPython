{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems with Python - Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/), [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/) and [Matthieu Brucher](https://www.linkedin.com/in/matthieubrucher/) published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are discussing two methods to reduce the feature space: filters and wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)\n",
    "    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "DPI = 300\n",
    "\n",
    "def save_png(name):\n",
    "    fn = 'B09124_05_%s.png'%name # please ignore, it just helps our publisher :-)\n",
    "    plt.savefig(os.path.join(CHART_DIR, fn), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting redundant features using filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr([1,2,3], [1,2,3.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr([1,2,3], [1,20,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def _plot_correlation_func(x, y):\n",
    "    r, p = pearsonr(x, y)\n",
    "    plt.scatter(x, y, c='b', s=15)\n",
    "    plt.title(\"Cor($X_1$, $X_2$) = %.3f\" % r)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    f1 = scipy.poly1d(scipy.polyfit(x, y, 1))\n",
    "    plt.plot(x, f1(x), \"r--\", linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # to reproduce the data later on\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(0, 10, 0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"01_corr_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While correlation is always a good start, it has its weaknesses with non-linear data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(-5, 5, 0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"02_corr_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(5, 4), dpi=DPI)\n",
    "\n",
    "plt.title(\"Entropy $H(X)$\")\n",
    "plt.xlabel(\"$P(X=$coin will show heads up$)$\")\n",
    "plt.ylabel(\"$H(X)$\")\n",
    "\n",
    "plt.xlim(xmin=0, xmax=1.1)\n",
    "x = np.arange(0.001, 1, 0.001)\n",
    "y = -x * np.log2(x) - (1 - x) * np.log2(1 - x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.ylim((0,1.05))\n",
    "plt.xlim((-.05,1.05))\n",
    "\n",
    "save_png('03_entropy_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mutual_info(x, y, bins=10):\n",
    "    counts_xy, bins_x, bins_y = np.histogram2d(x, y, bins=(bins, bins))\n",
    "    counts_x, bins = np.histogram(x, bins=bins)\n",
    "    counts_y, bins = np.histogram(y, bins=bins)\n",
    "\n",
    "    counts_xy += 1\n",
    "    counts_x += 1\n",
    "    counts_y += 1\n",
    "    P_xy = counts_xy / np.sum(counts_xy)\n",
    "    P_x = counts_x / np.sum(counts_x)\n",
    "    P_y = counts_y / np.sum(counts_y)\n",
    "\n",
    "    I_xy = np.sum(P_xy * np.log2(P_xy / (P_x.reshape(-1, 1) * P_y)))\n",
    "\n",
    "    return I_xy / (entropy(counts_x) + entropy(counts_y))\n",
    "\n",
    "def _plot_mi_func(x, y):\n",
    "    mi = normalized_mutual_info(x, y)\n",
    "    plt.scatter(x, y, s=15)\n",
    "    plt.title(\"NI($X_1$, $X_2$) = %.3f\" % mi)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "    \n",
    "\n",
    "np.random.seed(0)  # to reproduce the data later on\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(0, 10, 0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png('04_mi_demo_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(-5, 5, 0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png('05_mi_demo_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking the model about the features using wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100, n_features=10, n_informative=3, random_state=0)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "selector = RFE(clf, n_features_to_select=3)\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    selector = RFE(clf, i)\n",
    "    selector = selector.fit(X, y)\n",
    "    print(\"%i\\t%s\\t%s\" % (i, selector.support_, selector.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Princial Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "x1 = np.arange(0, 10, .2)\n",
    "x2 = x1 + np.random.normal(scale=1, size=len(x1))\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "x1 = np.arange(0, 10, .2)\n",
    "x2 = x1 + np.random.normal(scale=1, size=len(x1))\n",
    "\n",
    "good = (x1 > 5) | (x2 > 5)\n",
    "bad = ~good\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"06_pca_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of PCA and how Linear Discriminant Analysis (LDA) can help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider a data set where the labels aren't distributed according to the axis with the highest variance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "x1 = np.arange(0, 10, .2)\n",
    "x2 = x1 + np.random.normal(scale=1, size=len(x1))\n",
    "\n",
    "good = x1 > x2\n",
    "bad = ~good\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"07_pca_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that PCA doesn't work at all. LDA will still work fine here, because it takes the instance labels as additional input and is then able to minimize within-class distances in the reduced feature space while it maximizes samples that are in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "good = x1 > x2\n",
    "bad = ~good\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "lda_inst = LinearDiscriminantAnalysis(n_components=1)\n",
    "Xtrans = lda_inst.fit_transform(X, good)\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"08_lda_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, decomposition, datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# all examples will have three classes in this file\n",
    "colors = ['r', 'g', 'b']\n",
    "markers = ['o', 6, '*']\n",
    "\n",
    "\n",
    "X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "mds = manifold.MDS(n_components=3)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on example data set in 3 dimensions\")\n",
    "ax.view_init(10, -15)\n",
    "\n",
    "mds = manifold.MDS(n_components=2)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on example data set in 2 dimensions\")\n",
    "\n",
    "save_png(\"09_mds_demo_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# MDS\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "mds = manifold.MDS(n_components=3)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on Iris data set in 3 dimensions\")\n",
    "ax.view_init(10, -15)\n",
    "\n",
    "mds = manifold.MDS(n_components=2)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on Iris data set in 2 dimensions\")\n",
    "\n",
    "save_png(\"10_mds_demo_iris.png\")\n",
    "\n",
    "# PCA\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "Xtrans = pca.fit(X).transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"PCA on Iris data set in 3 dimensions\")\n",
    "ax.view_init(50, -35)\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"PCA on Iris data set in 2 dimensions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"11_pca_demo_iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "max = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's definine some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swissroll(n):\n",
    "    \"\"\"\n",
    "    Generates data for the swissroll\n",
    "    Returns the parameter space, the swissroll\n",
    "    \"\"\"\n",
    "    orig = np.random.random((2, n)) * max\n",
    "    return (orig.T, np.array((orig[1] * np.cos(orig[1]), orig[1] * np.sin(orig[1]), orig[0])).T)\n",
    "\n",
    "def color_from_parameters(params):\n",
    "    \"\"\"\n",
    "    Defines a colors scheme for the swissroll\n",
    "    \"\"\"\n",
    "    return np.array((params[:,0], params[:,1], max - params[:,1])).T / max\n",
    "\n",
    "def plot_3d(swissroll, colors):\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(swissroll[:,0], swissroll[:,2], swissroll[:,1], c=colors)\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Z axis')\n",
    "    ax.set_zlabel('Y axis')\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d(swissroll, colors):    \n",
    "    fig = plt.figure()\n",
    "    plt.scatter(swissroll[:,0], swissroll[:,1], c=colors)\n",
    "    plt.xlabel('Param 1')\n",
    "    plt.ylabel('Param 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to Tensorflow definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_create_variables():\n",
    "    swissroll_tf = tf.placeholder(tf.float32, (None, 3), name=\"swissroll\")\n",
    "    return swissroll_tf\n",
    "    \n",
    "def tf_create_dense_layer(x, size):\n",
    "    return tf.layers.dense(x, size, activation=tf.nn.leaky_relu, kernel_initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a class for the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self, swissroll, swissroll_test, nb_intermediate, learning_rate):\n",
    "        self.swissroll = swissroll\n",
    "        self.swissroll_test = swissroll_test\n",
    "        \n",
    "        self.swissroll_tf = tf_create_variables()\n",
    "        \n",
    "        intermediate_input = tf_create_dense_layer(self.swissroll_tf, nb_intermediate)\n",
    "        intermediate_input = tf_create_dense_layer(intermediate_input, nb_intermediate)\n",
    "        self.encoded = tf_create_dense_layer(intermediate_input, 2)\n",
    "        intermediate_output = tf_create_dense_layer(self.encoded, nb_intermediate)\n",
    "#        intermediate_output = tf_create_dense_layer(intermediate_output, nb_intermediate)\n",
    "        self.output = tf_create_dense_layer(intermediate_output, 3)\n",
    "    \n",
    "        self.meansq = tf.reduce_mean(tf.squared_difference(self.output, self.swissroll_tf))\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.meansq)\n",
    "    \n",
    "    def train(self, display, n_epochs, batch_size, **kwargs):\n",
    "        n = len(self.swissroll)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for i in range(n_epochs):\n",
    "                permut = np.random.permutation(n)\n",
    "                for j in range(0, n, batch_size):\n",
    "                    samples = permut[j:j+batch_size]\n",
    "                    batch = self.swissroll[samples]\n",
    "                    sess.run(self.train_step, feed_dict={self.swissroll_tf: batch})\n",
    "                if i % step == step - 1:\n",
    "                    print(\"Epoch :%i\\n  Loss %f\" % (i, sess.run(self.meansq, feed_dict={self.swissroll_tf: self.swissroll})))\n",
    "    \n",
    "            error = sess.run(self.meansq, feed_dict={self.swissroll_tf: self.swissroll})\n",
    "            error_test = sess.run(self.meansq, feed_dict={self.swissroll_tf: self.swissroll_test})\n",
    "            \n",
    "            if display:\n",
    "                pred = sess.run(self.encoded, feed_dict={self.swissroll_tf : self.swissroll})\n",
    "                pred = np.asarray(pred)\n",
    "                recons = sess.run(self.output, feed_dict={self.swissroll_tf : self.swissroll})\n",
    "                recons = np.asarray(recons)\n",
    "                recons_test = sess.run(self.output, feed_dict={self.swissroll_tf : self.swissroll_test})\n",
    "                recons_test = np.asarray(recons_test)\n",
    "            \n",
    "                print(\"Embedded manifold\")\n",
    "                plot_2d(pred, colors)\n",
    "                save_png(\"swissroll_embedded\")\n",
    "                print(\"Reconstructed manifold\")\n",
    "                plot_3d(recons, colors)\n",
    "                save_png(\"swissroll_reconstructed\")\n",
    "                print(\"Reconstructed test manifold\")\n",
    "                plot_3d(recons_test, kwargs['colors_test'])\n",
    "                save_png(\"swissroll_test\")\n",
    "        return error, error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n = 5000\n",
    "\n",
    "n_epochs = 2000\n",
    "batch_size = 100\n",
    "\n",
    "nb_intermediate = 20\n",
    "learning_rate = 0.05\n",
    "\n",
    "step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, swissroll = generate_swissroll(n)\n",
    "params_test, swissroll_test = generate_swissroll(n)\n",
    "colors = color_from_parameters(params)\n",
    "colors_test = color_from_parameters(params_test)\n",
    "print(\"Original manifold\")\n",
    "plot_3d(swissroll, colors)\n",
    "save_png(\"swissroll_original\")\n",
    "\n",
    "model = Autoencoder(swissroll, swissroll_test, nb_intermediate, learning_rate)\n",
    "\n",
    "error, error_test = model.train(True, n_epochs, batch_size, colors=colors, test=swissroll_test, colors_test = colors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
