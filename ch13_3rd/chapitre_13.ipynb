{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/), [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/) and [Matthieu Brucher](https://www.linkedin.com/in/matthieubrucher/) published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)\n",
    "\n",
    "def save_png(name):\n",
    "    fn = 'B09124_13_%s.png'%name # please ignore, it just helps our publisher :-)\n",
    "    plt.savefig(os.path.join(CHART_DIR, fn), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Q function the old fashion way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a table with some Q values for this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an empty table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "# Set learning hyperparameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "\n",
    "# Let's run!\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation (top left)\n",
    "    s = env.reset()\n",
    "    # Do 100 iterations to update the table\n",
    "    for i in range(100):\n",
    "        # Choose an action by picking the max of the table + additional random noise ponderated by the episode\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)//(i+1))\n",
    "        # Get new state and reward from environment after chosen step \n",
    "        s1, r, d,_ = env.step(a)\n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test games with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0.99\n",
    "e = 0.1 # 1 in 10 samples, we chose a new action for the network\n",
    "num_episodes = 2000\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# A simple one layer network\n",
    "inputs = tf.placeholder(shape=[None, 16], dtype=tf.float32, name=\"input\")\n",
    "Qout = tf.layers.dense(\n",
    "    inputs=inputs,\n",
    "    units=4,\n",
    "    use_bias=False,\n",
    "    name=\"dense\",\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=0, maxval=.0125)\n",
    ")\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "# Our optimizer will try to optimize \n",
    "nextQ = tf.placeholder(shape=[None, 4], dtype=tf.float32, name=\"target\")\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network, and check that it will get more and more sucesses as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of our games and our results\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        \n",
    "        for j in range(100):\n",
    "            a, targetQ = sess.run([predict, Qout], feed_dict={inputs:np.identity(16)[s:s+1]})\n",
    "            # We randomly choose a new state that we may have not encountered before\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            \n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs:np.identity(16)[s1:s1+1]})\n",
    "            # Obtain maxQ' and set our target value for chosen action.\n",
    "            targetQ[0, a[0]] = r + y*np.max(Q1)\n",
    "            \n",
    "            # Train our network using target and predicted Q values\n",
    "            sess.run(updateModel, feed_dict={inputs:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # Reduce chance of random action as we train the model.\n",
    "                e = 1 / ((i // 50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"Percent of succesful episodes: %f%%\" % (sum(rList) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the evolution of the reward with each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "plt.plot(lfilter(np.ones(20)/20, [1], rList))\n",
    "save_png(\"reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the survival increases, even if we take suoptimal paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jList)\n",
    "save_png(\"length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now design a a network that can tackle more or less any of the Atari games available on the gym plaform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import os\n",
    "import six\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few helper function, one to preprocess our images and shrink them and two others that will transpose the data. The reason is that we use the past images as additional channels, so the axis order is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))[None,:,:]\n",
    "\n",
    "def adapt_state(state):\n",
    "    return [np.float32(np.transpose(state, (2, 1, 0)) / 255.0)]\n",
    "\n",
    "def adapt_batch_state(state):\n",
    "    return np.transpose(np.array(state), (0, 3, 2, 1)) / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a bunch of hyperparameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_name = \"Breakout-v4\"\n",
    "\n",
    "width = 80  # Resized frame width\n",
    "height = 105  # Resized frame height\n",
    "\n",
    "n_episodes = 12000  # Number of runs for the agent\n",
    "state_length = 4  # Number of most frames we input to the network\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "exploration_steps = 1000000  # During all these steps, we progressively lower epsilon\n",
    "initial_epsilon = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "final_epsilon = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "\n",
    "initial_random_search = 20000  # Number of steps to populate the replay memory before training starts\n",
    "replay_memory_size = 400000  # Number of states we keep for training\n",
    "batch_size = 32  # Batch size\n",
    "network_update_interval = 10000  # The frequency with which the target network is updated\n",
    "train_skips = 4  # The agent selects 4 actions between successive updates\n",
    "\n",
    "learning_rate = 0.00025  # Learning rate used by RMSProp\n",
    "momentum = 0.95  # momentum used by RMSProp\n",
    "min_gradient = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "\n",
    "network_path = 'saved_networks/' + env_name\n",
    "tensorboard_path = 'summary/' + env_name\n",
    "save_interval = 300000  # The frequency with which the network is saved\n",
    "initial_quiet_steps = 10  # Initial steps while the agent is not doing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a class to train, save and restore our network.\n",
    "get_trained_action() will be the method used to get a new action from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, num_actions, restore_network=False):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_step = (initial_epsilon - final_epsilon) / exploration_steps\n",
    "        self.t = 0\n",
    "\n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.build_network(\"Q\")\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "\n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_network(\"Target\")\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        # Interactive session instead of the usual one just vecause it is simple to create\n",
    "        # Would need some refactoring otherwise of this constructor\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(tensorboard_path, self.sess.graph)\n",
    "\n",
    "        if not os.path.exists(network_path):\n",
    "            os.makedirs(network_path)\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "\n",
    "        if restore_network:\n",
    "            self.load_network()\n",
    "\n",
    "    def build_network(self, name):\n",
    "        model = tf.keras.Sequential(name=name)\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=32, kernel_size=8, strides=(4, 4), activation='relu', input_shape=(width, height, state_length), name=\"Layer1\" + name))\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=64, kernel_size=4, strides=(2, 2), activation='relu', name=\"Layer2\" + name))\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu', name=\"Layer3\" + name))\n",
    "        model.add(tf.keras.layers.Flatten(name=\"Flatten\" + name))\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu', name=\"Layer4\" + name))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, name=\"Output\" + name))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, width, height, state_length], name=\"state\" + name)\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        a = tf.placeholder(tf.int64, [None], name=\"actions\")\n",
    "        y = tf.placeholder(tf.float32, [None], name=\"qInput\")\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum, epsilon=min_gradient)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "    def get_initial_state(self, frame):\n",
    "        processed_frame = preprocess(frame)\n",
    "        state = [processed_frame for _ in range(state_length)]\n",
    "        return np.concatenate(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < initial_random_search:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: adapt_state(state)}))\n",
    "\n",
    "        # Decay epsilon over time\n",
    "        if self.epsilon > final_epsilon and self.t >= initial_random_search:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run(self, state, action, reward, terminal, frame):\n",
    "        next_state = np.append(state[1:, :, :], frame, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        if len(self.replay_memory) > replay_memory_size:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        if self.t >= initial_random_search:\n",
    "            # Train network\n",
    "            if self.t % train_skips == 0:\n",
    "                self.train_network()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % network_update_interval == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % save_interval == 0:\n",
    "                save_path = self.saver.save(self.sess, network_path + '/' + env_name, global_step=self.t)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: adapt_state(state)}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Write summary\n",
    "            stats = [self.total_reward, self.total_q_max / self.duration,\n",
    "                    self.duration, self.total_loss / (self.duration / train_skips)]\n",
    "            for i in range(len(stats)):\n",
    "                self.sess.run(self.update_ops[i], feed_dict={\n",
    "                    self.summary_placeholders[i]: float(stats[i])\n",
    "                })\n",
    "            summary_str = self.sess.run(self.summary_op)\n",
    "            self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def train_network(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, batch_size)\n",
    "        for data in minibatch:\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: adapt_batch_state(next_state_batch)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * gamma * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: adapt_batch_state(next_state_batch),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0., name=\"EpisodeTotalReward\")\n",
    "        tf.summary.scalar(env_name + '/Total_Reward/Episode', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0., name=\"EpisodeAvgMaxQ\")\n",
    "        tf.summary.scalar(env_name + '/Average_Max Q/Episode', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0., name=\"EpisodeDuration\")\n",
    "        tf.summary.scalar(env_name + '/Duration/Episode', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0., name=\"EpisodeAverageLoss\")\n",
    "        tf.summary.scalar(env_name + '/Average_Loss/Episode', episode_avg_loss)\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "    def load_network(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(network_path)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Training new network...')\n",
    "\n",
    "    def get_trained_action(self, state):\n",
    "        action = np.argmax(self.q_values.eval(feed_dict={self.s: adapt_state(state)}))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our network (and save some final images from the trained network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make(env_name)\n",
    "agent = Agent(num_actions=env.action_space.n)\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    terminal = False\n",
    "    frame = env.reset()\n",
    "    for _ in range(random.randint(1, initial_quiet_steps)):\n",
    "        frame, _, _, _ = env.step(0)  # Do nothing\n",
    "    state = agent.get_initial_state(frame)\n",
    "    while not terminal:\n",
    "        action = agent.get_action(state)\n",
    "        frame, reward, terminal, _ = env.step(action)\n",
    "\n",
    "        processed_frame = preprocess(frame)\n",
    "        state = agent.run(state, action, reward, terminal, processed_frame)\n",
    "    env.env.ale.saveScreenPNG(six.b('%s/test_image_%05i.png' % (CHART_DIR, i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
